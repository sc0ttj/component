<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<script src="../dist/component.min.js"></script>
<script src="../dist/useAudio.min.js"></script>
</head>
<body>

  <h1>Click here to load the sounds..</h1>
  <p>(Also look in the DevTools "Console" for more info)</p>

  <br>
  <p class="log"></p>

<script>

  function log(msg) {
    document.querySelector('.log').innerText += `* ${msg}\n`;
  }

  document.body.addEventListener('click', function clicker() {
    document.body.removeEventListener('click', clicker);
    log('loading sounds..');

    // attach the useAudio add-on to Component
    Component.useAudio = useAudio;

    // we can now create and manage separate audio libraries, by attaching the
    // audio to different components

    // Example 1: simple example of loading multiple songs
    const Soundtrack = new Component({});

    // define your sounds
    Soundtrack.useAudio({
      song1: 'sounds/sirens.mp3',  // TODO
      //song2: 'sounds/song2.mp3', // - allow passing in <audio>/<video> Elements, to use their src as inputs
      //song3: 'sounds/song3.mp3', // - allow passing in multiple URLs (array of strings) for multiple filetypes & better browser support
    });

    // -------------------------------------------

    // Example 2: advanced example
    const Effects = new Component({});

    // define your sounds
    Effects.useAudio({
      // add some sound assets, as above
      //ok: 'sounds/ok.mp3',
      //back: 'sounds/back.mp3',
      //exit: 'sounds/exit.mp3',
      // add sound assets with custom properties filters and callbacks:
      heroVoice: {
        src: 'sounds/speech.mp3',
        volume: 0.20,
        loop: false,
        playbackRate: 1,    // 1 is normal speed, 2 is double speed, etc
        fadeIn: 0,          // give a duration, in seconds, like 0.2
        filters: {
          delay: 0,         // give a duration, in seconds, like 0.2
          panning: -1,      // -1 is left, 0 is center, 1 is right
//          lowshelf:  { freq:  400, gain: 0.2 },
//          highshelf: { freq:  1200, gain: 0.2 },
//          lowpass:   { freq: 400, gain: 0.1 },
//          bandpass:  { freq:  800, gain: 0.2, q: 0.5 },
//          highpass:  { freq: 1200, gain: -1 },
//          notch:     { freq:  800, gain: 0.2, q: 0.5 },
//          peaking:   { freq:  800, gain: 0.2, q: 0.5 },
//          equalizer: [
//            { freq:  200,  q: 0.25, gain: 0.9 }, // lowpass filter
//            { freq:  800,  q: 0.25, gain: 0.9 }, // peaking filter(s) (can have many)
//            { freq:  1200, q: 0.25, gain: 0.9 }, // highpass filter
//          ],
//          reverb: {
//            duration: 1,
//            decay: 1,
//            reverse: true,
//          },
          // how much to randomise various properties each time a sound is played
//          randomization: {
//            volume: 0.8,
//            playbackRate: 0.6,
//            startOffset: 0.0001,
//            delay: 0.01,
//          },
          // this enables the analyser node, useful for visualizations (see below)
          analyser: {
            fftSize: 2048,
            minDecibels: -100,
            maxDecibels: -30,
            smoothingTimeConstant: 0.8,
          },
          compression: {
            threshold: -50.0,
            knee: 40.0,
            ratio: 12.0,
            attack: 0.0,
            release: 0.25,
          },
        },
        // lots of callbacks are available
        onPlay: props => console.log(props),   // props is the current state of the sound,
        onPause: props => console.log(props),  // and includes all settings for the filters
        onResume: props => console.log(props), // that you have enabled
        onStop: props => console.log(props),
      },
      // you can use another sound object, or an array of them as sources:
      // this will channel the existing sounds into the panNode of the new sound
      // (instead of audioCtx.destination).. when you play track1, then song1 and
      // song2 wil play together.. you can edit the settings of track1 to change
      // its sound output, without affecting the settings of its sources
      //..?track1: {
      //  src: [ song1, song2 ],
      //},
    });

    document.addEventListener('audioLoaded', function audioLoaded(e) {
      document.removeEventListener('audioLoaded', audioLoaded);
      log('sounds loaded');

      // can use the sounds in here
      console.log('==========================================');
      console.log('Sounds loaded:');
      console.log('Soundtrack', Soundtrack.audio);
      console.log('Effects', Effects.audio);
      console.log('==========================================');

      // use your sounds like so
      //Soundtrack.audio.song1.play();

      // or extract sounds into their own variables
      const { song1 } = Soundtrack.audio;
      // and use them

      // Using your sounds
      //Effects.audio.ok.play();

      // or, grab an audio library as a variable
      const effects = Effects.audio;
      // or grab individual sounds as variables
      //const { ok, back, exit } = effects;
      // then use the methods on each sound..
      //ok.play();
      //ok.pause();
      // etc..

      // update a sounds properties using the settings() method, which is like a
      // setState for sound objects - pass in only the properties you want to change
      //ok.settings({ volume: 0.25 });

      const { heroVoice } = effects;

      // name settings objects to easily create "presets"
      const quiet = { volume: 0.08 };
      song1.settings(quiet);

      log('playing sound..');
      document.querySelector('h1').innerText = 'Playing sounds and adding effects..';

      //song1.play();

      // you can re-route sounds into other sounds.. this will connect song1 to
      // the "pan" node of heroVoice:
      song1.connectTo(heroVoice);

      console.log('song1', song1);

      console.log('>> first play');
      heroVoice.play();

      setTimeout(() => {
        console.log('................updating panning..........' );
        log('panning to center');
        heroVoice.settings({ panning: 0 });
        console.log('heroVoice', heroVoice);
      }, 1500);

      setTimeout(() => {
        console.log('................increasing volume..........' );
        log('increasing volume');
        heroVoice.settings({ volume: 0.7 });
      }, 4000);

      setTimeout(() => {
        console.log('................fade out sirens..........' );
        log('start fade out of sirens, over 5 seconds');
        song1.fadeOut(5);
      }, 7000);


      // after 4 seconds, pause for 2 seconds, then resume
      //setTimeout(function() {
      //  console.log('pause');
      //  heroVoice.pause();

      //  setTimeout(function() {
      //    console.log('resume');
      //    heroVoice.play();
      //  }, 3000);
      //}, 3000);

     setTimeout(() => {
       console.log('................updating equaliser..........' );
       log('add equaliser');
       heroVoice.settings({
         equalizer: [
           { freq:  400, q: 0.1, gain: 0.002 },   // lowspass filter
           { freq:  1200, q: 1.0, gain: 0.6 },    // peaking filter(s) (can have many)
           { freq:  1500, q: 1.25, gain: 0.001 }, // highpass filter
         ],
       });
       console.log('heroVoice', heroVoice);

     }, 10000);

      setTimeout(() => {
        console.log('................updating reverb..........' );
        log('adding reverb');
        heroVoice.settings({
          volume: 1,
          reverb: {
            duration: 0.8,
            decay: 0.9,
            reverse: false,
          },
        });
        console.log('heroVoice', heroVoice);
      }, 13000);

      setTimeout(() => {
        console.log('................updating randomization settings..........' );
        log('randomizing playback rate');
        heroVoice.settings({
         randomization: {
           playbackRate: 0.6,
         },
        });
        console.log('heroVoice', heroVoice);
      }, 16000);

      setTimeout(() => {
        log('fade out (over 10 seconds)');
        heroVoice.fadeOut(10);
      }, 20000);




      // you can also update the properties of all sounds at once - call
      // settings() on the library object itself, instead of on its sounds
      //effects.settings({
      //  volume: 0.85,
      //});


      // to do visualisations, you should use the "visualiser" and "visualData"
      // properties of your sound object (note: the analyser needs to be enabled
      // in the sounds settings for these to exist)

      // define a function, running on a loop, that powers our visualisation, it
      // should receive the props (current state) of a sound object
      const visualisation = (props) => {
        return; // remove this line to enable it
        requestAnimationFrame(visualisation);
        // get our analyser node ("visualiser") and some useful analyser data
        const { visualiser, visualData, bufferLength } = props;
        // get data suitable for generating waveforms
        const waveData = visualiser.getByteTimeDomainData(visualData);
        // or get data suitable for bars/equalisers
        const barData = visualiser.getByteFrequencyData(visualData);
        //...now do your visualisation(s)
        for (var i = 0; i < bufferLength; i++) {
          // do stuff here
          return;
        }
      };

      // we want our visualisation to start when our sound plays
      heroVoice.onPlay = props => {
        visualisation(props);
      };
      heroVoice.onResume = props => {
        visualisation(props);
      };
      // and let's stop the visualisation animation when the sound stops
      heroVoice.onStop = props => {
        cancelAnimationFrame(visualisation);
      };
      heroVoice.onPause = props => {
        cancelAnimationFrame(visualisation);
      };

    });

   /*
    * Audio UI components:  useAudioUI add-on
    *
    *    const Foo = new Component({})
    *
    *    Foo.useAudio({ ... });
    *
    *    Foo.audio.sound1.controls('.some-container')
    *
    *    // the above would add a UI to the page with controls (inputs,
    *    // sliders, etc) for all enabled settings.
    */

    /*
     * Routing sounds into other sounds?
     *
     * Mixer example:
     *
     *    const Mixer = new Component({})
     *
     *    Mixer.useAudio({
     *      track1: [ foo, bar ],    // output foo and bar to track1, instead of audioCtx.destination
     *      track2: [ song1 ],       // output song1 to track2, instead of audioCtx.destination
     *    })
     *
     *    const { track1 } = Mixer.audio
     *
     *    track1.settings({ ... })    // dont change foo or bar, only the gain/filter/etc nodes after
     *
     */

   });

</script>
</body>

</html>
