<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<script src="../dist/component.min.js"></script>
<script src="../dist/useAudio.min.js"></script>
</head>
<body>

  <h1>Click here to load the sounds..</h1>
  <p>(Also look in the DevTools "Console" for more info)</p>

  <br>
  <p class="log"></p>

<script>

   // define a custom logging function
  function log(msg) {
    document.querySelector('.log').innerText += `* ${msg}\n`;
  }


  // according to the Web Audio spec, users must interact with the page before
  // a sound can be played, so lets just add a "click to load sounds" event
  document.body.addEventListener('click', function clicker() {

    // remove the click event once page has been clicked, on order to only
    // load the sounds once
    document.body.removeEventListener('click', clicker);

    log('loading sounds..');

    // attach the useAudio add-on to Component
    Component.useAudio = useAudio;

    // we can now create and manage separate audio libraries, by attaching the
    // audio to different components

    // Example 1: simple example of loading multiple songs
    const Soundtrack = new Component({});

    // define your sounds
    Soundtrack.useAudio({
      song1: 'sounds/sirens.mp3', // the path to the file
    });

    // -------------------------------------------

    // Example 2: advanced example
    const Effects = new Component({});

    // define your sounds
    Effects.useAudio({
      // lets add sound assets with custom properties filters and callbacks:
      heroVoice: {
        src: 'sounds/speech.mp3', // the path to the file
        volume: 0.20,
        loop: false,
        playbackRate: 1,    // 1 is normal speed, 2 is double speed, etc
        filters: {
          delay: 0,         // give a duration, in seconds, like 0.2
          panning: -1,      // -1 is left, 0 is center, 1 is right
          // add any combination of "biquad" filters
          // (see https://developer.mozilla.org/en-US/docs/Web/API/BiquadFilterNode)
//          lowshelf:  { freq:  400, gain: 0.2 },
//          highshelf: { freq: 1200, gain: 0.2 },
//          lowpass:   { freq:  400, q: 0.1 },
//          highpass:  { freq: 1200, q: 0.1 },
//          allpass:   { freq: 1200, q: 0.1 },
//          bandpass:  { freq:  800, q: 0.5 },
//          peaking:   { freq:  800, gain: 0.2, q: 0.5 },
//          notch:     { freq:  800, q: 0.5 },
//          // or add an array of filters into a customisable "equalizer"
//          equalizer: [
//            { freq:  200,  q: 0.25, gain: 0.9 }, // lowpass filter
//            { freq:  800,  q: 0.25, gain: 0.9 }, // peaking filter(s) (can have many)
//            { freq:  1200, q: 0.25, gain: 0.9 }, // highpass filter
//          ],
//          reverb: {
//            duration: 1,
//            decay: 1,
//            reverse: true,
//          },
//          // how much to randomise various properties each time a sound is played
//          randomization: {
//            volume: 0.8,
//            playbackRate: 0.6,
//            startOffset: 0.0001,
//            delay: 0.01,
//          },
          // this enables the analyser node, useful for visualizations (see below)
          analyser: {
            fftSize: 2048,
            minDecibels: -100,
            maxDecibels: -30,
            smoothingTimeConstant: 0.8,
          },
          compression: {
            threshold: -50.0,
            knee: 40.0,
            ratio: 12.0,
            attack: 0.0,
            release: 0.25,
          },
        },
        // lots of callbacks are available
        onPlay: props => console.log('onPlay()', props),     // props is the current state of the sound,
        onPause: props => console.log('onPause()', props),   // and includes all settings for the filters
        onResume: props => console.log('onResume()', props), // that you have enabled
        onStop: props => console.log('onStop()', props),
      },
    });


    // the "audioLoaded" event fires when *all* sounds, from all components,
    // have finished loading
    document.addEventListener('audioLoaded', function audioLoaded(e) {

      // only load this event once - remove it once loaded
      document.removeEventListener('audioLoaded', audioLoaded);

      log('sounds loaded');

      // now we can use these sounds:
      console.log('==========================================');
      console.log('Sounds loaded:');
      console.log('Soundtrack', Soundtrack.audio);
      console.log('Effects', Effects.audio);
      console.log('==========================================');

      // you can extract the whole audio library as a variable
      const effects = Effects.audio;

      // or extract sounds into their own variables
      const { song1 } = Soundtrack.audio;
      const { heroVoice } = effects;

      // name settings objects to easily create "presets"
      const quiet = { volume: 0.08 };

      // update a sounds properties using the settings() method, which is like a
      // setState for sound objects - pass in only the properties you want to change
      song1.settings(quiet);

      // you can also update the properties of all sounds at once - call
      // settings() on the library object itself, instead of on its sounds
      effects.settings({
        volume: 0.25,
      });

      // you can re-route sounds into other sounds.. this will connect song1 to
      // the "pan" node of heroVoice:
      song1.connectTo(heroVoice);


      // update title on page, so user can see sound is now playing
      document.querySelector('h1').innerText = 'Playing sounds and adding effects..';

      log('playing sound..');
      heroVoice.play();
      console.log('heroVoice', heroVoice);

      //
      // lets update the sound as it's playing...
      //

      setTimeout(() => {
        console.log('................updating panning..........' );
        log('panning to center');
        heroVoice.settings({ panning: 0 });
      }, 1500);

      setTimeout(() => {
        console.log('................increasing volume..........' );
        log('increasing volume');
        heroVoice.settings({ volume: 0.7 });
      }, 4000);

      setTimeout(() => {
        console.log('................fade out sirens..........' );
        log('start fade out of sirens, over 5 seconds');
        song1.fadeOut(5);
      }, 7000);


      //setTimeout(function() {
      //  console.log('pause');
      //  heroVoice.pause();

      //  setTimeout(function() {
      //    console.log('resume');
      //    heroVoice.play();
      //  }, 9000);
      //}, 8000);

     setTimeout(() => {
       console.log('................updating equaliser..........' );
       log('add equaliser');
       heroVoice.settings({
         equalizer: [
           { freq:  400, q: 0.1, gain: 0.002 },   // lowspass filter
           { freq:  1200, q: 1.0, gain: 0.6 },    // peaking filter(s) (can have many)
           { freq:  1500, q: 1.25, gain: 0.001 }, // highpass filter
         ],
       });
     }, 11000);

      setTimeout(() => {
        console.log('................adding reverb..........' );
        log('adding reverb');
        heroVoice.settings({
          volume: 1,
          reverb: {
            duration: 0.8,
            decay: 0.9,
            reverse: false,
          },
        });
      }, 13000);

      setTimeout(() => {
        console.log('................removing reverb..........' );
        log('removing reverb');
        heroVoice.settings({ reverb: false });
      }, 17000);

      setTimeout(() => {
        console.log('................updating randomization settings..........' );
        log('randomizing playback rate');
        heroVoice.settings({
         randomization: {
           playbackRate: 0.6,
         },
        });
      }, 20000);

      setTimeout(() => {
        log('fade out (over 10 seconds)');
        heroVoice.fadeOut(10);
      }, 24000);


      // to do visualisations, you should use the "visualiser" and "visualData"
      // properties of your sound object (note: the analyser needs to be enabled
      // in the sounds settings for these to exist)

      // define a function, running on a loop, that powers our visualisation, it
      // should receive the props (current state) of a sound object
      const visualisation = (props) => {
        return; // remove this line to enable it
        requestAnimationFrame(visualisation);
        // get our analyser node ("visualiser") and some useful analyser data
        const { visualiser, visualData, bufferLength } = props;
        // get data suitable for generating waveforms
        const waveData = visualiser.getByteTimeDomainData(visualData);
        // or get data suitable for bars/equalisers
        const barData = visualiser.getByteFrequencyData(visualData);
        //...now do your visualisation(s)
        for (var i = 0; i < bufferLength; i++) {
          // do stuff here
          return;
        }
      };

      // we want our visualisation to start when our sound plays
      heroVoice.onPlay = props => {
        visualisation(props);
      };
      heroVoice.onResume = props => {
        visualisation(props);
      };
      // and let's stop the visualisation animation when the sound stops
      heroVoice.onStop = props => {
        cancelAnimationFrame(visualisation);
      };
      heroVoice.onPause = props => {
        cancelAnimationFrame(visualisation);
      };


    });


   });

</script>
</body>

</html>
