<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<script src="../dist/component.min.js"></script>
<script src="../dist/useAudio.min.js"></script>
</head>
<body>

  <h1>Click here to load the sounds..</h1>

  <p class="log"></p>

<script>

  function log(msg) {
    document.querySelector('.log').innerText += `* ${msg}\n`;
  }

  document.body.addEventListener('click', () => {

    log('loading sounds..');

    // attach the useAudio add-on to Component
    Component.useAudio = useAudio;

    // we can now create and manage separate audio libraries, by attaching the
    // audio to different components

    // Example 1: simple example of loading multiple songs
    const Soundtrack = new Component({});

    // define your sounds
    Soundtrack.useAudio({
      song1: 'sounds/sirens.mp3',  // TODO
      //song2: 'sounds/song2.mp3', // - allow passing in <audio>/<video> Elements, to use their src as inputs
      //song3: 'sounds/song3.mp3', // - allow passing in multiple URLs (array of strings) for multiple filetypes & better browser support
    });

    // -------------------------------------------

    // Example 2: advanced example
    const Effects = new Component({});

    // define your sounds
    Effects.useAudio({
      // add some sound assets, as above
      //ok: 'sounds/ok.mp3',
      //back: 'sounds/back.mp3',
      //exit: 'sounds/exit.mp3',
      // add sound assets with custom properties filters and callbacks:
      heroVoice: {
        src: 'sounds/speech.mp3',
        volume: 0.001,
        loop: false,
        playbackRate: 1,    // 1 is normal speed, 2 is double speed, etc
        fadeIn: 0,          // give a duration, in seconds, like 0.2
        filters: {
          delay: 0,         // give a duration, in seconds, like 0.2
          panning: -1,      // -1 is left, 0 is center, 1 is right
//          lowshelf:  { freq:  400, gain: 0.2 },
//          highshelf: { freq:  1200, gain: 0.2 },
//          lowpass:   { freq: 400, gain: 0.1 },
//          bandpass:  { freq:  800, gain: 0.2, q: 0.5 },
//          highpass:  { freq: 1200, gain: -1 },
//          notch:     { freq:  800, gain: 0.2, q: 0.5 },
//          peaking:   { freq:  800, gain: 0.2, q: 0.5 },
//          equalizer: [
//            { freq:  200, q: 1 },       // lowpass filter
//            { freq:  800, q: 0.25 },    // peaking filter(s) (can have many)
//            { freq:  1200, q: 0.25 },   // highpass filter
//          ],
//          reverb: {
//            duration: 1,
//            decay: 1,
//            reverse: true,
//          },
          // how much to randomise various properties each time a sound is played
//          randomization: {
//            volume: 0.8,
//            playbackRate: 0.6,
//            startOffset: 0.0001,
//            delay: 0.01,
//          },
          // this enables the analyser node, useful for visualizations (see below)
          analyser: {
            fftSize: 2048,
            minDecibels: -100,
            maxDecibels: -30,
            smoothingTimeConstant: 0.8,
          },
          compression: {
            threshold: -50.0,
            knee: 40.0,
            ratio: 12.0,
            attack: 0.0,
            release: 0.25,
          },
        },
        // lots of callbacks are available
        onPlay: props => console.log(props),   // props is the current state of the sound,
        onPause: props => console.log(props),  // and includes all settings for the filters
        onResume: props => console.log(props), // that you have enabled
        onStop: props => console.log(props),
      },
      // you can use another sound object, or an array of them as sources:
      // this will channel the existing sounds into the panNode of the new sound
      // (instead of audioCtx.destination).. when you play track1, then song1 and
      // song2 wil play together.. you can edit the settings of track1 to change
      // its sound output, without affecting the settings of its sources
      //..?track1: {
      //  src: [ song1, song2 ],
      //},
    });

    document.addEventListener('audioLoaded', function(e) {
      log('sounds loaded');

      // can use the sounds in here
      console.log('==========================================');
      console.log('Sounds loaded:');
      console.log('Soundtrack', Soundtrack.audio);
      console.log('Effects', Effects.audio);
      console.log('==========================================');

      // use your sounds like so
      //Soundtrack.audio.song1.play();

      // or extract sounds into their own variables
      //const { song2, song3 } = Soundtrack.audio;
      // and use them
      //song2.play();


      // Using your sounds
      //Effects.audio.ok.play();

      // or, grab an audio library as a variable
      const effects = Effects.audio;
      // or grab individual sounds as variables
      //const { ok, back, exit } = effects;
      // then use the methods on each sound..
      //ok.play();
      //ok.pause();
      // etc..

      // update a sounds properties using the settings() method, which is like a
      // setState for sound objects - pass in only the properties you want to change
      //ok.settings({ volume: 0.25 });

      const { heroVoice } = effects;

      // or name settings objects to easily create "presets"
      //const outside = { volume: 0.7 };
      //heroVoice.settings(outside);

      // or combine them
      //heroVoice.settings({
      //  ...outside,
      //  fadeIn: 1,
      //});

       log('playing sound..');

       heroVoice.play();

       setTimeout(() => {
         console.log('................increasing volume..........' );
         log('increasing volume');
         heroVoice.settings({ volume: 0.7 });
         console.log('heroVoice', heroVoice);
       }, 100);

       setTimeout(() => {
         console.log('................updating panning..........' );
         log('panning to center');
         heroVoice.settings({
           panning: 0,
         });
         console.log('heroVoice', heroVoice);
       }, 2500);

       setTimeout(() => {
         console.log('................updating equaliser..........' );
         log('add equaliser');
         heroVoice.settings({
           equalizer: [
             { freq:  400, q: 0.1, gain: 0.002 },   // lowspass filter
             { freq:  1200, q: 1, gain: 0.6 },      // peaking filter(s) (can have many)
             { freq:  1500, q: 1.25, gain: 0.001 }, // highpass filter
           ],
         });
         console.log('heroVoice', heroVoice);
       }, 5500);

       setTimeout(() => {
         console.log('................updating reverb..........' );
         log('adding reverb');
         heroVoice.settings({
           reverb: {
             duration: 0.8,
             decay: 0.9,
             reverse: false,
           },
         });
         console.log('heroVoice', heroVoice);
       }, 9000);

       setTimeout(() => {
         console.log('................updating randomization settings..........' );
         log('randomizing playback rate');
         heroVoice.settings({
          randomization: {
            playbackRate: 0.6,
          },
         });
         console.log('heroVoice', heroVoice);
       }, 11000);

       setTimeout(() => {
         log('fade out (over 5 seconds)');
         heroVoice.fadeOut(5);
       }, 18000);

//      // after 4 seconds, pause for 2 seconds, then resume
//      setTimeout(function() {
//
//        setTimeout(function() {
//          console.log('pause');
//          heroVoice.pause();
//        }, 5000);
//
//        setTimeout(function() {
//          console.log('resume');
//          heroVoice.play();
//        }, 7000);
//
//     }, 4000);



      // you can also update the properties of all sounds at once - call
      // settings() on the library object itself, instead of on its sounds
      //effects.settings({
      //  volume: 0.85,
      //});


      //console.log('heroVoice', heroVoice);


      // or you can re-route sounds into other sounds.. this will connect song1 to
      // the "pan" node of track1:
      //song1.connect(track1);


      // to do visualisations, you should use the "visualiser" and "visualData"
      // properties of your sound object (note: the analyser needs to be enabled
      // in the sounds settings for these to exist)

      // define a function, running on a loop, that powers our visualisation, it
      // should receive the props (current state) of a sound object
      const visualisation = (props) => {
        return; // remove this line to enable it
        requestAnimationFrame(visualisation);
        // get our analyser node ("visualiser") and some useful analyser data
        const { visualiser, visualData, bufferLength } = props;
        // get data suitable for generating waveforms
        const waveData = visualiser.getByteTimeDomainData(visualData);
        // or get data suitable for bars/equalisers
        const barData = visualiser.getByteFrequencyData(visualData);
        //...now do your visualisation(s)
        for (var i = 0; i < bufferLength; i++) {
          // do stuff here
          return;
        }
      };

      // we want our visualisation to start when our sound plays
      heroVoice.onPlay = props => {
        visualisation(props);
      };
      heroVoice.onResume = props => {
        visualisation(props);
      };
      // and let's stop the visualisation animation when the sound stops
      heroVoice.onStop = props => {
        cancelAnimationFrame(visualisation);
      };
      heroVoice.onPause = props => {
        cancelAnimationFrame(visualisation);
      };

    });

   /*
    * Audio UI components:  useAudioUI add-on
    *
    *    const Foo = new Component({})
    *
    *    Foo.useAudio({ ... });
    *
    *    Foo.audio.sound1.controls('.some-container')
    *
    *    // the above would add a UI to the page with controls (inputs,
    *    // sliders, etc) for all enabled settings.
    */

    /*
     * Routing sounds into other sounds?
     *
     * Mixer example:
     *
     *    const Mixer = new Component({})
     *
     *    Mixer.useAudio({
     *      track1: [ foo, bar ],    // output foo and bar to track1, instead of audioCtx.destination
     *      track2: [ song1 ],       // output song1 to track2, instead of audioCtx.destination
     *    })
     *
     *    const { track1 } = Mixer.audio
     *
     *    track1.settings({ ... })    // dont change foo or bar, only the gain/filter/etc nodes after
     *
     */

   });

</script>
</body>

</html>
